# 4 Clustering and classification

<br>

### 4.1. Exploring the dataset 
The assignment 4 started with loading the *The Boston Housing*; `Boston` dataset. The dataset is openly available from the MASS-package in R. Below I look at the structure with `str()` and use `summary()` to see the details of the variables. 

<br>

```{r, warning=FALSE, message=FALSE}
# access the MASS package and all the other libraries needed for this Assignment
library(MASS)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)

# load the data
data("Boston")

# explore the dataset structure, summary and missing data
str(Boston)
summary(Boston)
sum(is.na(Boston))

```
<br>

The dataset contains 506 cases and has 14 variables (=columns). There is 12 numeric and 2 integer-valued variables and no missing data.

Variable      meaning       
----------    ----------
crim          per capita crime rate by town
zn            proportion of residential land zoned for lots over 25,000 sq.ft.
indus         proportion of non-retail business acres per town.
chas          Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox           nitrogen oxides concentration (parts per 10 million).
rm            average number of rooms per dwelling.
age           proportion of owner-occupied units built prior to 1940.
dis           weighted distances to five Boston employment centres
rad           index of accessibility to radial highways
tax           full-value property-tax rate per $10,000
ptratio       pupil-teacher ratio by town
black         1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
lstat         lower status of the population (%)
medv          median value of owner-occupied homes in $1000's

<br>

When we study the data a bit more, we can see that the variables are not normally distributed:

```{r, warning=FALSE, message=FALSE}
# plots of variables
gather(Boston) %>% ggplot(aes(value)) + 
  geom_density() + 
  facet_wrap("key", scales = "free") + 
  ggtitle("'Boston' datasetin muuttujat")

```
<br>

Below is a plot of correlation matrix to look at the pairwise correlations of the variables:
```{r}
# calculate the correlation matrix and round it
cor_matrix <- Boston %>% cor() %>% round(2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="square", order = "AOE", type = "lower", tl.srt = 45, col = COL2("PRGn", 10), diag = FALSE)

```
<br>

Looking at the visualization, we can see that at least **nox**, **indus**, **rad** and **dis** have quite high absolute pairwise correlations with other variables. It is not suprising that for example the NO-levels in air (**nox**) correlates with proportion of non-retail business acres per town (**indus**), accessibility to highways (**rad**) correlates with property taxes (**tax**) and median value of homes (**medv**) correlates with the percentage of the lower status population in the area (**lstat**).

<br>

### 4.2. Standardizing the dataset

Below I'll scale the data by subtracting the column means from the corresponding columns and dividing the difference with standard deviation. This data standardization transforms the data with a mean of 0 and a standard deviation of 1.  

```{r, warning=FALSE, message=FALSE}
# center and standardize variables 
boston_scaled <- as.data.frame(scale(Boston))
                          
# summaries of the scaled variables
summary(boston_scaled)

```
<br>
As we can see, the variables have a mean of 0 and a standard deviation of 1. 
<br>

Next I'll create a categorical variable of the crime rate in the Boston dataset using quantiles as the break points. I'll also remove the original variable **crim** from the scaled dataset and add new categorical value. Lastly I'll print out the table of the new categorical variable **crime**.

```{r, warning=FALSE, message=FALSE}
# create a quantile vector of crim 
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# look at the table of the new factor crime
table(crime)

```
Now we have a categorical variable of crime rate; each of the four groups being the same size.

<br>

### 4.3. Dividing the dataset

Below I'll divide the dataset into 'train' and 'test' sets, so that random 80 % of the data is in the 'train' set and the rest is in the 'test' set.

```{r, warning=FALSE, message=FALSE}
# number of rows in the Boston dataset (needed to define 80 % of the rows)
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```
<br>

### 4.4. Linear discriminant analysis

Next I'll fit linear discriminant analysis (LDA) to the dataset. Linear discriminant analysis finds the (linear) combination of the variables that separate the target variable classes. The target can be binary or multiclass variable. Here I'm using categorical **crime** as a target variable and the rest of the variables as predictors.

```{r, warning=FALSE, message=FALSE}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results 
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```
<br>
Percentage separations achieved by the first discriminant function is 95 %, second is 3.7 % and the third is 1.3 %. Biplot based on LD1 and LD2 shows that **high** is mostly separated from others around LD1 6 and LD2 0 (only few **med_high** overlapping), when other groups have more overlap. Based on arrows, **rad** explains clearly the **high** group, whereas other predictors are in the middle.  

<br>

Next I'll predict the classes with LDA model on the test data and cross tabulate the results with the crime categories from the test dataset. 

```{r, warning=FALSE, message=FALSE}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```
<br>
There are total of $18+16+13+28=75$ correct predictions, which is 73 % of the 102 observations. Compared to the accuracy of 25 % without our model, the 73 % is quite good. The **high** class gets predicted correct at almost all cases (28/30). It seems that the model is not that good predicting low and med_low classes. 

<br>

### 4.5. K-means clustering

Below I'll load the 'Boston' dataset again and scale it. Then I'll calculate the Euclidean and the Manhattan distance on it. 

```{r, warning=FALSE, message=FALSE}
data("Boston")
# center and standardize variables
boston_scaled2 <- as.data.frame(scale(Boston))

# euclidean distance matrix
dist_eu <- dist(boston_scaled2)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled2, method="manhattan")

# look at the summary of the distances
summary(dist_man)

```
<br> 

Next I'll run k-means algorithm on the dataset 'boston_scaled2'. 


```{r, warning=FALSE, message=FALSE}
# k-means clustering
km <- kmeans(boston_scaled2, centers = 4)

# plot the Boston dataset with clusters (first 7 variables)
pairs(boston_scaled2[1:7], col = km$cluster)

# plot the Boston dataset with clusters (last 7 variables)
pairs(boston_scaled2[8:14], col = km$cluster)
```

It can be seen from the visualizations, that the clusters overlap quite a lot. 

<br>


K-means needs the number of clusters as an argument and next I'll investigate that. I'll determine the number of clusters by looking at how the total of within cluster sum of squares (WCSS) behaves when the number of cluster changes. When plotting the number of clusters and the total WCSS, the optimal number of clusters is when the total WCSS drops radically.

I'll use the function `set.seed()` to deal with possibly different results every time.


```{r, warning=FALSE, message=FALSE}
set.seed(123)

# determine the number of clusters to be 10
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```
<br>

The line always declines with the count of clusters and the idea is to identify where is a appropriate number of clusters (where is the breaking point of the most added value from an additional cluster). It seems that the line starts to level off at around two, and after that the line continues roughly on the same angle. Therefore I decided on using two clusters and fit the model again. Below we can see that it is much easier to distinguish the two clusters from each other. 

```{r, warning=FALSE, message=FALSE}
set.seed(123)

# k-means clustering
km <- kmeans(boston_scaled2, centers = 2)

# plot the Boston dataset with clusters (first 7 variables)
pairs(boston_scaled2[1:7], col = km$cluster)
# plot the Boston dataset with clusters (last 7 variables)
pairs(boston_scaled2[8:14], col = km$cluster)

```

<br> 

### 4.6. Data Wrangling
Data wrangling for the next week can be found [here.](https://github.com/kreikka/IODS-project/blob/master/data/create_human.R)

<br> 

```{r, warning=FALSE, message=FALSE}
# Document last knitted:
date()
```